{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+--------------------+------+\n",
      "|                 url|parsed|\n",
      "+--------------------+------+\n",
      "|https://h_amazon....| false|\n",
      "+--------------------+------+\n",
      "\n",
      "Tour  1\n",
      "+--------------------+------+\n",
      "|                 url|parsed|\n",
      "+--------------------+------+\n",
      "|https://h_amazon....| false|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "class sparkTest:\n",
    "    def __init__(self):\n",
    "        self.sparkSession = SparkSession.builder.appName(\"shud\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "        self.sqlContext = SQLContext(self.sparkSession)    \n",
    "\n",
    "        \n",
    "        \n",
    "    def mainProcess(self):\n",
    "        \"\"\"Initialize the processing\"\"\"\n",
    "        initList = [(\"https://h_amazon.com/DealsHomeUrl\", \"false\")]\n",
    "        df = self.sparkSession.createDataFrame(initList, schema=[\"url\", \"parsed\"])\n",
    "        self.sqlContext.registerDataFrameAsTable(df, \"UrlList\")\n",
    "\n",
    "        df = self.sqlContext.sql(\"SELECT url, parsed from UrlList where parsed = 'false'\")\n",
    "        indx = 0\n",
    "        print(len(df.rdd.collect()))\n",
    "        for url in df.rdd.collect():\n",
    "            self.parse(url[0])\n",
    "            df.show()\n",
    "            df2 = self.sqlContext.sql(\"SELECT url, parsed from UrlList where parsed = 'false'\")\n",
    "            df = df.union(df2).dropDuplicates()\n",
    "            if indx > 10:\n",
    "                break\n",
    "            indx +=1\n",
    "        while len(df.rdd.collect()) > 0:\n",
    "            print(\"Tour \", indx)\n",
    "            df.show()\n",
    "            for url in df.rdd.collect():\n",
    "                self.parse(url[0])                \n",
    "            \n",
    "            df = self.sqlContext.sql(\"SELECT url, parsed from UrlList where parsed = 'false'\")\n",
    "            \n",
    "            if indx > 20:\n",
    "                break\n",
    "            indx +=1\n",
    "            \n",
    "        \n",
    "    def parse(self, url):\n",
    "        # Parse url and add newly found urls\n",
    "        newUrls = []\n",
    "        #newUrls.append((\"https://x1_amazon.com/x1\", \"false\"))\n",
    "        #newUrls.append((\"https://x2_amazon.com/x2\", \"false\"))\n",
    "        \n",
    "        #Set current url as parsed\n",
    "        newUrls.append((url, \"true\"))\n",
    "        \n",
    "        #Get all urls to synchronize and update\n",
    "        df = self.sqlContext\\\n",
    "            .sql(\"SELECT url, parsed from UrlList where url <>'%s'\" % url)\\\n",
    "            .union(self.sparkSession.createDataFrame(newUrls))\\\n",
    "            .union(self.sparkSession.createDataFrame([(url, \"true\")]))\\\n",
    "            .dropDuplicates(['url'])\n",
    "        \n",
    "        self.sqlContext.registerDataFrameAsTable(df, \"UrlList\")\n",
    "        \n",
    "        \n",
    "\n",
    "#Testing\n",
    "st = sparkTest()\n",
    "st.mainProcess()\n",
    "\n",
    "\n",
    "def x():\n",
    "    df = df.union(spark.createDataFrame([(\"https://x1_amazon.com/xx\", \"false\")]))\n",
    "    df = df.union(spark.createDataFrame([(\"https://x2_amazon.com/xx\", \"false\")]))\n",
    "    df = df.union(spark.createDataFrame([(\"https://x1_amazon.com/xx\", \"false\")]))\n",
    "    #df.show()\n",
    "    sqlContext.registerDataFrameAsTable(df, \"UrlList\")\n",
    "    #tt = \"table1\" in sqlContext.tableNames()\n",
    "    #df2 = sqlContext.sql(\"SELECT url, parsed from UrlList where parsed = 'false'\")\n",
    "    #df2.collect()\n",
    "    l = ['x','y']\n",
    "    df = spark.createDataFrame((l, 'false'), schema=[\"url\", \"parsed\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
