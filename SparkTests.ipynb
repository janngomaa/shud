{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "class ShudCrawler:\n",
    "    def __init__(self):\n",
    "        self.sparkSession = SparkSession.builder\\\n",
    "            .appName(\"shud\")\\\n",
    "            .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "            .getOrCreate()\n",
    "        self.sqlContext = SQLContext(self.sparkSession)    \n",
    "\n",
    "        \n",
    "        \n",
    "    def mainProcess(self):\n",
    "        \"\"\"Initialize the processing\"\"\"\n",
    "        initList = [(\"https://h_amazon.com/DealsHomeUrl\", \"false\")]\n",
    "        #initList.append((\"https://x1_amazon.com/x1\", \"false\"))\n",
    "        #initList.append((\"https://x2_amazon.com/x2\", \"false\"))\n",
    "        \n",
    "        df = self.sparkSession.createDataFrame(initList, schema=[\"url\", \"parsed\"])\n",
    "        \n",
    "        self.sqlContext.registerDataFrameAsTable(df, \"UrlList\")\n",
    "\n",
    "        df = self.sqlContext.sql(\"SELECT url, parsed from UrlList where parsed = 'false'\")\n",
    "        \n",
    "        '''\n",
    "        print(len(df.rdd.collect()))\n",
    "        for url in df.rdd.collect():\n",
    "            self.parse(url[0])\n",
    "            df.show()\n",
    "            df2 = self.sqlContext.sql(\"SELECT url, parsed from UrlList where parsed = 'false'\")\n",
    "            df = df.union(df2).dropDuplicates()\n",
    "            if indx > 10:\n",
    "                break\n",
    "            indx +=1\n",
    "        '''\n",
    "        \n",
    "        indx = 0\n",
    "        \n",
    "        while len(df.rdd.collect()) > 0:\n",
    "            print(\"Tour \", indx)\n",
    "            for url in df.rdd.collect():\n",
    "                self.parse(url[0], indx)                \n",
    "            \n",
    "            df = self.sqlContext.sql(\"SELECT url, parsed from UrlList where parsed = 'false'\")\n",
    "            \n",
    "            if indx > 3:\n",
    "                break\n",
    "            indx +=1\n",
    "            \n",
    "        \n",
    "    def parse(self, url, indx):\n",
    "        # Parse url and add newly found urls\n",
    "        newUrls = []\n",
    "        #newUrls.append((\"https://x1_amazon.com/x1\", \"false\"))\n",
    "        if indx < 6:\n",
    "            newUrls.append((\"https://x\" + str(indx) + \"_amazon.com/x\"+ str(indx), \"false\"))\n",
    "        \n",
    "        #Set current url as parsed\n",
    "        newUrls.append((url, \"true\"))\n",
    "        \n",
    "        #Get all urls to synchronize and update\n",
    "        df = self.sqlContext\\\n",
    "            .sql(\"SELECT url, parsed from UrlList where url <>'%s'\" % url)\\\n",
    "            .union(self.sparkSession.createDataFrame(newUrls))\\\n",
    "            .union(self.sparkSession.createDataFrame([(url, \"true\")]))\\\n",
    "            .dropDuplicates(['url'])\n",
    "        \n",
    "        self.sqlContext.registerDataFrameAsTable(df, \"UrlList\")\n",
    "        \n",
    "        df.show()\n",
    "        \n",
    "        \n",
    "\n",
    "#Testing\n",
    "st = ShudCrawler()\n",
    "st.mainProcess()\n",
    "\n",
    "\n",
    "def x():\n",
    "    '''df = df.union(spark.createDataFrame([(\"https://x1_amazon.com/xx\", \"false\")]))\n",
    "    df = df.union(spark.createDataFrame([(\"https://x2_amazon.com/xx\", \"false\")]))\n",
    "    df = df.union(spark.createDataFrame([(\"https://x1_amazon.com/xx\", \"false\")]))\n",
    "    #df.show()\n",
    "    sqlContext.registerDataFrameAsTable(df, \"UrlList\")\n",
    "    #tt = \"table1\" in sqlContext.tableNames()\n",
    "    #df2 = sqlContext.sql(\"SELECT url, parsed from UrlList where parsed = 'false'\")\n",
    "    #df2.collect()\n",
    "    l = ['x','y']\n",
    "    df = spark.createDataFrame((l, 'false'), schema=[\"url\", \"parsed\"])\n",
    "    '''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|let|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "|  3|  c|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "sparkSession = SparkSession.builder\\\n",
    "            .appName(\"ShudSparkSession\")\\\n",
    "            .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "            .getOrCreate()\n",
    "            \n",
    "sqlContext = SQLContext(sparkSession) \n",
    "l = [(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "df = sparkSession.createDataFrame(l, schema=[\"id\", \"let\"])\n",
    "sqlContext.registerDataFrameAsTable(df, \"Letters\")\n",
    "\n",
    "class classX():\n",
    "    def __init__(self):\n",
    "        self.sparkSession = SparkSession.builder\\\n",
    "            .appName(\"ShudSparkSession\")\\\n",
    "            .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "            .getOrCreate()\n",
    "            \n",
    "        self.sqlContext = SQLContext(sparkSession) \n",
    "    def process(self):\n",
    "        df = self.sqlContext.sql(\"SELECT id, let from Letters\")\n",
    "        df.show()\n",
    "        \n",
    "        \n",
    "x = classX()\n",
    "x.process()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Shud Crawler\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "sparkSession = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ShudSparkSession\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "sqlContext = SQLContext(sparkSession)\n",
    "df = sqlContext.sql(\"SELECT * from WorkTable\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import str\n",
    "\n",
    "sc = SparkContext('local[2]', 'pyspark tutorial') \n",
    "\n",
    "lines = sc.textFile(response.url)\n",
    "lines.union(sc.textFile(response.url))\n",
    "\n",
    "def noPunctuations(text):\n",
    "    \"\"\"Removes punctuation and convert to lower case\n",
    "    Args:\n",
    "        text (str): A string.\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation)).lower()\n",
    "\n",
    "lines_rdd = sc.textFile(get(1), 1)\n",
    "counts = lines_rdd.map(noPunctuations).flatMap(lambda x: x.split(' ')) \\\n",
    "         .map(lambda x: (x, 1)) \\\n",
    "\t\t .reduceByKey(lambda x, y: x+y)\n",
    "\t\t \n",
    "for (word, count) in counts.collect():\n",
    "   print(word,count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
