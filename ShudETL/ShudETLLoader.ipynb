{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'couchbase'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8e5437dc4fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSourceFileLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#couchabse packages import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcouchbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcouchbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPasswordAuthenticator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcouchbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn1ql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mN1QLQuery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'couchbase'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "from pyspark import *\n",
    "import configparser\n",
    "import json\n",
    "from pprint import pprint\n",
    "#import rhinoscriptsyntax as rs\n",
    "import os\n",
    "import json_lines\n",
    "import pandas as pd\n",
    "import pandasql as pdsql\n",
    "import hashlib\n",
    "import sys\n",
    "#import importlib.util\n",
    "from importlib.machinery import SourceFileLoader\n",
    "#couchabse packages import\n",
    "from couchbase.bucket import Bucket\n",
    "from couchbase.cluster import Cluster, PasswordAuthenticator\n",
    "from couchbase.n1ql import N1QLQuery\n",
    "import couchbase._libcouchbase as LCB\n",
    "import couchbase.exceptions as E\n",
    "from couchbase.user_constants import FMT_JSON\n",
    "from couchbase._pyport import ulp\n",
    "\n",
    "class ShudETLLoader:\n",
    "    def __init__(self):\n",
    "        self.helper = SourceFileLoader(\"ShudHelper\", \"/home/jovyan/work/shud/helper/helper.py\").load_module()\n",
    "        self.config = self.helper.ShudHelper.getConfig() \n",
    "        self.sparkSession = SparkSession\\\n",
    "                .builder\\\n",
    "                .appName('appname')\\\n",
    "                .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "                .getOrCreate()\n",
    "        self.sqlContext = SQLContext(self.sparkSession)\n",
    "        self.imagedirectory=self.config.get('directory', 'imagedirectory')\n",
    "        self.datadirectory=self.config.get('directory', 'datadirectory')\n",
    "\n",
    "        print(self.datadirectory)\n",
    "    def loadData(self):\n",
    "        files=self.helper.getJsonFiles(self.datadirectory)\n",
    "        pushdata={}\n",
    "        for file in files:\n",
    "            df = self.sparkSession.read.json(file)\n",
    "            df.createOrReplaceTempView(\"df\")\n",
    "            sqlDF = self.sparkSession.sql(\"SELECT * FROM df\")           \n",
    "            for row in sqlDF.rdd.collect():\n",
    "                Enable=1\n",
    "                Language='fr'\n",
    "                Description='description'\n",
    "                image_urls=row[\"image_urls\"]\n",
    "                images={}\n",
    "                for image_url in image_urls:\n",
    "                    sha_1 = hashlib.sha1()\n",
    "                    sha_1.update(image_url)\n",
    "                    imageID=sha_1.hexdigest()\n",
    "                    imagePath=self.imagedirectory + imageID +\".jpg\"\n",
    "                   # Image=self.helper.getImageBase64(imagePath)\n",
    "                    images[imageID]=imagePath                    \n",
    "                    \n",
    "                productdetail={'productid': row[\"id\"], 'createdat': row[\"domain\"], 'updatedate': datetime.datetime.now(), \n",
    "                            'ProductDescription': {'descriptionid':row[\"id\"], 'productName': row[\"title\"],'Description': Description, 'Language': Language,'CreatedAt':datetime.datetime.now(), 'UpdatedAt':datetime.datetime.now()},\n",
    "                            'ProductImage': {'Productid': row[\"id\"], 'ImageID':row[\"id\"], 'ImageLink':[images], 'CreatedAt':datetime.datetime.now(), 'UpdatedAt':datetime.datetime.now()}, 'Enable': Enable }\n",
    "                \n",
    "                pushdata[row[\"id\"]]=productdetail        \n",
    "        ########load data\n",
    "        self.helper.InsertData(DBName='ShudDB',BucketName='product', Data=pushdata)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/shudStaging/data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ShudETLLoader at 0x7fb2c7acd780>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShudETLLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<configparser.ConfigParser object at 0x7fb2ec084828>\n"
     ]
    }
   ],
   "source": [
    "#import importlib.util\n",
    "#'../shud.ini'\n",
    "\n",
    "from importlib.machinery import SourceFileLoader\n",
    "\n",
    "#helper = SourceFileLoader(\"ShudHelper\", \"/home/jovyan/work/shud/helper/helper.py\").load_module()\n",
    "#helper.ShudHelper()\n",
    "\n",
    "#x=helper.ShudHelper.cleanHtml('self', '/home/jovyan/work/shud.ini')\n",
    "#print(x)\n",
    "#import sys\n",
    "\n",
    "#sys.path.append('home/jovyan/work/shud/helper')\n",
    "helper = SourceFileLoader(\"ShudHelper\", \"/home/jovyan/work/shud/helper/helper.py\").load_module()\n",
    "config = helper.ShudHelper.getConfig()\n",
    "helper.ShudHelper.cleanHtml('aaa')\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "    def DBkey(self, bucket):\n",
    "        cluster = Cluster('couchbase://localhost')\n",
    "        cluster.authenticate(PasswordAuthenticator('DAG', 'jayceelyndsey07'))\n",
    "        cb = cluster.open_bucket(bucket)\n",
    "        ExistID = cb.n1ql_query(N1QLQuery('SELECT id FROM bucket'))\n",
    "        print(ExistID)\n",
    "        plain_text=[]\n",
    "        for key in ExistID:\n",
    "            cipher_suite = Fernet(key)\n",
    "            plain_text.append(cipher_suite.decrypt(key))\n",
    "        while True:\n",
    "            key = Fernet.generate_key()\n",
    "            cipher_suite = Fernet(key)\n",
    "            x=str(uuid.uuid4())\n",
    "            cipher_text = cipher_suite.encrypt(x)\n",
    "            if (cipher_text not in plain_text) :\n",
    "                break\n",
    "        return cipher_text           \n",
    "path = \"grp_20180420_005907.942949.jl\"\n",
    "\n",
    "\n",
    "#df.select(df['id']).show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sqlDFpro=sparkSession.sql(\"SELECT domain, title as ProductName, current_timestamp FROM df\")\n",
    "#sqlDF.show()\n",
    "#df.rdd.map(lambda x: (x.id, x.domain, x.ProductName))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#sc = spark.sparkContext\n",
    "\n",
    "path = \"shudStaging/grp_20180420_005907.942949.jl\"\n",
    "df = sparkSession.read.json(path)\n",
    "\n",
    "#df.select(df['id']).show()\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "sqlDF = sparkSession.sql(\"SELECT * FROM df\")\n",
    "sqlDFpro=sparkSession.sql(\"SELECT domain, title as ProductName, current_timestamp FROM df\")\n",
    "#sqlDFpro.show()\n",
    "sqlDF.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''''\n",
    "product\n",
    "productid: uuid\n",
    "createdat: domain\n",
    "updatedate: sysdate\n",
    "descriptionid: uuid\n",
    "productName: title\n",
    "''''\n",
    "#df.show(5)\n",
    "#pip install json-lines\n",
    "#pip install pandasql\n",
    "data = []\n",
    "with open(path, 'rb') as json_data:\n",
    "\t\tfor item in json_lines.reader(json_data):\n",
    "\t\t\t#print(item)\n",
    "\t\t\tdata.append(item)\n",
    "\t\tjson_data.close()\n",
    "\t#print data\n",
    "df1 = pd.io.json.json_normalize(data)\n",
    "df1.columns = df1.columns.map(lambda x: x.split(\".\")[-1])\n",
    "\n",
    "str1=\"select id from df1\"\n",
    "#result=pdsql.sqldf(str1, locals())\n",
    "#result.head(10)\n",
    "#sqldf=sparkSession.sql(\"SELECT * FROM df\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
